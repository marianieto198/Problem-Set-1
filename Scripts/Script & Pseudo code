#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#%    Data Acquisition    		          %
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
##Paquetes 
rm(list = ls())



require("pacman")
p_load(devtools)
p_load(ggpubr)
devtools::install_github("thomasp85/patchwork")
p_load(rvest)
library(ggplot2)
library(patchwork)
p_load(rio) 
p_load(tidyverse)
p_load(e1071) 
p_load(EnvStats) 
p_load(tidymodels) 
p_load(ggplot2) 
p_load(scales) 
p_load(ggpubr) 
p_load(knitr) 
p_load(kableExtra)
p_load(broom)
p_load(caret)


#1. En un primer momento se explora el link para verificar la manera en la que está presentada la información
#En este caso, la muestra de GEIH se presenta en una tabla fija, por lo que es más fácil realizar el scrape.

#2. Ahora, con el uso del click derecho se inspecciona la página y se obtiene el url de acceso para el código html desde la ventana de network
#Se realiza un loop para cada chunk de valores

#3. Se define la base vacía o la base inicial que será llenada por la información scrapeada de los url
#Pagina1 <- read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_1.html") %>%
#  html_table()  
#class(Pagina1) #La variable es tipo List y toca volverla Data Frame

#Pagina1 <- as.data.frame(Pagina1) # Volvemos la lista un Data Frame
df <- data.frame()



#3.1.Para cada chunk de información se extrae un temporal en donde se irá almacenando la información de cada chunk. 
#Para este paso se utiliza el comando “for” junto con el comando “paste0” para concatenar los diferentes url en un único elemento. 
for (i in 1:10) {
  url <- paste0("https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_",i,".html") #Vamos iterando la pagina con i
  chunk_i <- read_html(url)  %>% 
    html_table()  # Se crea la lista del chunk
  chunk_i <- as.data.frame(chunk_i) #se vuelve data frame el chunk
  #3.2.Se utiliza el comando rbind para pegar las filas de información almacenadas en cada archivo temporal y unificarlas en un único elemento.
  df <-  rbind(df, chunk_i) 
}

geih <- df # le cambiamos el nombre
###Otra opción es la siguiente:

#1.
#Crear una lista vacia en donde se van a almacenar las tablas tomadas del sitio web.
#data=list()
#Loop para importar las 10 tablas y añadir a la lista data
#for (i in 1:10){
#  url=paste0('https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_',i,'.html')
#  temp=read_html(url) %>%
#    html_table()
#  data=append(data,temp)
#}

#Loop para concatenar las bases de datos y formar GEIH
#geih=data.frame()
#for (i in 1:10){
#  geih=rbind(geih,data[[i]])
#}

#Se evidencia que no hay restricciones para acceder a los datos y realizar el scrapping, lo anterior por lo que se
#mencionó con anterioridad de que al ser chunks de información fijos facilita la lectura y extracción de los datos.

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#%    Data Cleaning   		          %
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

##Ya al tener lista la base de datos GEIH contamos con 178 variables y 32177 observaciones.
##Ahora bien, se empezará analizando las dimensiones de la base 
dim(geih)
str(geih)
##Ahora se corrigen las variables para que sean leídas como categóricas, bulein o string dependiendo del caso. 
##Lo anterior, ya que en su mayoría fueron leídas como enteros.
##Se definen entonces las variables categóricas

geih <- geih %>%
  mutate_at(.vars = c(
    "cclasnr11", "cclasnr2", "cclasnr3", "cclasnr4", "cclasnr5",
    "cclasnr6", "cclasnr7", "cclasnr8", "clase", "college",
    "cotPension", "cuentaPropia", "depto", "directorio", "dominio",
    "dsi", "estrato1", "formal", "ina", "inac", "informal",
    "maxEducLevel", "p6050", "microEmpresa", "ocu", "oficio", 
    "orden", "p6090", "p6100", "p6210", "p6210s1", "p6240", "p6510",
    "p6510s2", "p6545", "p6545s2", "p6580", "p6580s2", "p6585s1",
    "p6585s1a2", "p6585s2", "p6585s2a2", "p6585s4", "p6585s4a2",
    "p6590", "p6610", "p6620", "p6630s1", "p6630s2", "p6630s3",
    "p6630s4", "p6630s6", "p6920", "p7040", "p7050", "p7090",
    "p7110", "p7120", "p7140s1", "p7140s2", "p7150", "p7160",
    "p7310", "p7350", "p7422", "p7472", "p7495", "p7500s1",
    "p7500s2", "p7500s3", "p7505", "p7510s1", "p7510s2",
    "p7510s3", "p7510s5", "p7510s6", "p7510s7", "pea", "pet", 
    "regSalud", "relab", "secuencia_p", "sex", "sizeFirm", "wap"),
    .funs = factor)

str(geih)
summary (geih) # se puede ver que hay un gran número de observaciones vacias

#Se elimina la primera variable que no es necesaria
geih <- geih %>%
  select(-Var.1)

#Ahora se necesita hacer el filtro de la base de datos por personas ocupadas, que vivan en Bogotá y que sean mayores de 18 años
geih <- geih %>%
  filter(ocu == 1,
         dominio == "BOGOTA",
         age > 18)

dim(geih) # La base pasó de 32177 observaciones a 16397 observaciones
summary(df) #se puede observar que aunque bajaron los NAs sigue habiendo una presencia significativa en muchas variables


##Tratamiento de missing values

cantidad_na <- sapply(geih, function(x) sum(is.na(x))) #Una función que me suma el número de NAs por variable
cantidad_na <- data.frame(cantidad_na) #Lo convierto en Data Frame
porcentaje_na <- cantidad_na/nrow(geih) #Le saco el porcentaje de Missing values a cada variable

# Porcentaje de observaciones faltantes. 
porcentaje <- mean(porcentaje_na[,1]) #El 45.21% de las variables tiene NAs
print(paste0("En promedio el ", round(porcentaje*100, 2), "% de las entradas están vacías"))

##Ordenamos de mayor a menor
porcentaje_na <- arrange(porcentaje_na, desc(cantidad_na))
# Convertimos el nombre de la fila en columna
porcentaje_na <- rownames_to_column(porcentaje_na, "variable")

# Quitamos las variables que no tienen NAs
filtro <- porcentaje_na$cantidad_na == 0
variables_sin_na <- porcentaje_na[filtro, "variable"]
str_count(variables_sin_na) #Hay 58 variables sin NA
variables_sin_na <- paste(variables_sin_na, collapse = ", ")
print(paste("Las variables sin NAs son:", variables_sin_na))

porcentaje_na <- porcentaje_na[!filtro,] #Quedan solo 119 variables con NAs

orden <- porcentaje_na$variable[length(porcentaje_na$variable):1] #Se vuelven caracteres
porcentaje_na$variable <- factor(porcentaje_na$variable,
                                 levels = orden) #Se utilizan como factores para poder graficar

str(porcentaje_na) # Se revisa el tipo de variables

# Como son tantas variables vamos a hacer una gráfica con los que tienen menos NAs
#para analizar si se pueden imputar los valores

ggplot(porcentaje_na[101:nrow(porcentaje_na),], 
       aes(y = variable, x = cantidad_na)) +
  geom_bar(stat = "identity", fill = "darkslategray3") +
  geom_text(aes(label = paste0(round(100*cantidad_na, 1), "%")),
            colour = "white", position = "dodge", hjust = 1.3,
            size = 2, fontface = "bold") +
  theme_classic() +
  labs(x = "Porcentaje de NAs", y = "Variables") +
  scale_x_continuous(labels = scales::percent, limits = c(0, 1))


#Hay un salto de 36% de NAs de la variable ie a 10.6% de la variable  y_total_m 

#Si la cantidad de missing values es superior al 5% no se pueden imputar los datos (crearlos a partir de la media o la moda)
##Se eliminan las variables que tienen más del 5% de NAs
filtro2 <- porcentaje_na$cantidad_na > 0.05
variables_eliminadas <- porcentaje_na$variable[filtro2]
geih_clean <- geih %>%
  select(-variables_eliminadas) 
k0 <- ncol(geih)
k1 <- ncol(geih_clean)
print(paste("Se eliminaron", k0-k1, "variables. Ahora la base tiene", k1, "columnas."))
#Ahora solo tenemos 62 variables

porcentaje_na %>%
  filter(cantidad_na <= 0.05)
#Nos quedan 4 variables con NAs: impa, p7070, isa, maxEducLevel


#Como tres variables son continuas se reemplaza por la mediana, para que queden en el percentil 50% 
geih_clean <- geih_clean %>%
  mutate(impa = ifelse(is.na(impa), median(impa, na.rm = T), impa), #Mediana sin tener en cuenta los NAs
         p7070 = ifelse(is.na(p7070), median(p7070, na.rm = T), p7070),
         isa = ifelse(is.na(isa), median(isa, na.rm = T), isa))


sum(is.na(geih_clean$impa)) #Reviso
sum(is.na(geih_clean$p7070)) #Reviso
sum(is.na(geih_clean$isa)) #Reviso

#Para MaxLevelEduc toca sacar la moda y eso lo hacemos mirando su plot y después reemplazando
ggplot(geih_clean, aes(x = maxEducLevel)) + geom_bar()

#La moda es la categoría 7
moda_MaxEduc <- which(table(geih_clean$maxEducLevel) == max(table(geih_clean$maxEducLevel))) #Guardo el mayor valor
filtro2 <- is.na(geih_clean$maxEducLevel) #filtro solo los NAs
geih_clean$maxEducLevel[filtro2] <- moda_MaxEduc #Reemplazo los NAs por la moda
table(geih_clean$maxEducLevel) #Reviso

sum(is.na(geih_clean$maxEducLevel)) #Reviso

geih_clean %>% #Se revisa que todo el DF esté sin NAs
  is.na() %>%
  sum()

#En adición hay ingresos que no se ven reflajados en el ingreso total. Por tal motivo se actualizará
#esta variable incluyendo los ingresos del último mes no registrados
geih_clean <- geih_clean %>%
  mutate(ingtot = ifelse(ingtot == 0, p7500s1a1 + p7500s2a1 + p7500s3a1 + impa +
                           isa , ingtot))


##Existe la opción de normalizar los datos, se agregará el código de manera demostrativa
#Sin embargo, no se nirmalizarán los datos para facilitar su interpretación.

# Primero seleccionamos las columnas numéricas
#filtro <- sapply(geih, is.numeric)
#geih_escalada <- geih
#geih_escalada[,filtro] <- scale(geih[,filtro])

# Podemos obtener las medias y las desviaciones con las que se hizo la transformación
#lista <- attributes(geih_escalada)
#medias <- lista$`scaled:center`
#desviaciones <-  lista$`scaled:scale`

# Visualicemos los resultados de media y desviación

# Media
#apply(geih_escalada[,filtro], MARGIN = 2, 
#     function(x) round(mean(x, na.rm = T), 2))

#apply(geih_escalada[,filtro], MARGIN = 2, 
#     function(x) round(sd(x, na.rm = T), 2))

#head(geih_escalada)


##Transformaciones para resolver asimetrías

# Primero seleccionamos las columnas numéricas
filtro <- sapply(geih_clean, is.numeric)
sk <- sapply(geih_clean[, filtro], skewness, na.rm = T)

# Vamos a buscar las variables con las mayores asimetrías sin importar la dirección
sk_abs <- abs(sk)
sk_abs <- data.frame(sk_abs)
sk_abs <- rownames_to_column(sk_abs, "variable")
sk_abs <- arrange(sk_abs, desc(sk_abs))

# Las variables con mayor asimetría son:
head(sk_abs)

head(sk_abs)

p_load(e1071)

hist(geih_clean$ingtot)

pload(e1071)

# Construimos el vector de los ingresos y eliminamos los NAs 
x <- geih_clean$ingtot[!is.na(geih$ingtot)]
x <- geih_clean$ingtot[geih_clean$ingtot!=0]
sk_x <- skewness(x)
print(paste("El valor de skewness para los ingresos totales es", round(sk_x, 2)))

x <- log(geih_clean$ingtot[!is.na(geih_clean$ingtot)])
x <- geih_clean$ingtot[geih_clean$ingtot!=0]
sk_x <- skewness(x)
print(paste("El valor de skewness para los ingresos totales es", round(sk_x, 2)))

# Vamos a aplicar diferentes transformaciones y a visualizar como cambia el skewness
# Encontrar lambda óptimo
lambda <- boxcox(x, objective.name = "Log-Likelihood", 
                 optimize = TRUE)$lambda
box_cox_x <- boxcoxTransform(x, lambda)

ing_tot <- data.frame("Ingresos totales" = x,
                      "Logaritmo" = log(x),
                      "Raiz cuadrada" = sqrt(x),
                      "Inversa" = 1/x,
                      "Box-Cox" = box_cox_x)


# Observemos la distribución original
p1 <- ggplot(ing_tot) +
  geom_histogram(aes(x = Ingresos.totales, 
                     fill = "Ingresos totales"), 
                 alpha = 0.5, fill = "gray", bins = 30) +
  geom_text(aes(x = Inf, y = Inf, hjust = 1, 
                vjust = 1, 
                label = paste("Skewness", round(sk_x, 2)))) +
  theme_classic() +
  labs(x = "Ingresos totales", y = "Cantidad") +
  scale_x_continuous(labels = scales::dollar)

sk_x2 <- skewness(ing_tot$Logaritmo)
p2 <- ggplot(ing_tot) +
  geom_histogram(aes(x = Logaritmo, 
                     fill = "Logaritmo"), 
                 alpha = 0.5, fill = "blue", bins = 30) +
  geom_text(aes(x = Inf, y = Inf, hjust = 1, 
                vjust = 1, 
                label = paste("Skewness", round(sk_x2, 2)))) +
  theme_classic() +
  labs(x = "Log(Ingresos totales)", y = "Cantidad") 

sk_x3 <- skewness(ing_tot$Raiz.cuadrada)
p3 <- ggplot(ing_tot) +
  geom_histogram(aes(x = Raiz.cuadrada, 
                     fill = "Raíz cuadrada"), 
                 alpha = 0.5, fill = "red", bins = 30) +
  geom_text(aes(x = Inf, y = Inf, hjust = 1, 
                vjust = 1, 
                label = paste("Skewness", round(sk_x3, 2)))) +
  theme_classic() +
  labs(x = "Raiz cuadrada de Ingresos totales", y = "Cantidad") 

sk_x4 <- skewness(ing_tot$Inversa)
p4 <- ggplot(ing_tot) +
  geom_histogram(aes(x = Inversa, 
                     fill = "Inversa"), 
                 alpha = 0.5, fill = "green", bins = 30) +
  geom_text(aes(x = Inf, y = Inf, hjust = 1, 
                vjust = 1, 
                label = paste("Skewness", round(sk_x4, 2)))) +
  theme_classic() +
  labs(x = "1/(Ingresos totales)", y = "Cantidad")

sk_x5 <- skewness(ing_tot$Box.Cox)
p5 <- ggplot(ing_tot) +
  geom_histogram(aes(x = Box.Cox, 
                     fill = "Box-Cox"), 
                 alpha = 0.5, fill = "purple", bins = 30) +
  geom_text(aes(x = Inf, y = Inf, hjust = 1, 
                vjust = 1, 
                label = paste("Skewness", round(sk_x5, 2)))) +
  theme_classic() +
  labs(x = "Transformacion Box-Cox de Ingresos totales", 
       y = "Cantidad")

ggarrange(p1, p2, p3, p4, p5, nrow = 3, ncol = 2)

##U otra opción es 

(p1 | p2 | p3) /
  (p4 | p5)


##En búsqueda de valores atípicos se realizan gráficos de cajas y bigotes y análisis de correlaciones

##Distribución horas trabajadas
d1 <- ggplot(geih_clean, aes(y = totalHoursWorked)) +
  geom_boxplot(fill = "darkslategray3", alpha = 0.5) +
  theme_classic() +
  labs(y = "Horas trabajadas") +
  scale_x_discrete( ) 

##Distribución ingresos
d2 <- ggplot(geih_clean, aes(y = ingtot)) +
  geom_boxplot(fill = "darkslategray3", alpha = 0.5) +
  theme_classic() +
  labs(y = "Ingresos totales") +
  scale_y_continuous(labels = scales::dollar) +
  scale_x_discrete( ) 

##Distribución experiencia
d3 <- ggplot(geih_clean, aes(y = p6426)) +
  geom_boxplot(fill = "darkslategray3", alpha = 0.5) +
  theme_classic() +
  labs(y = "Años de Experiencia") +
  scale_y_continuous(labels = scales::dollar) +
  scale_x_discrete( ) 

##Distribución edad
d4 <- ggplot(geih_clean, aes(y = age)) +
  geom_boxplot(fill = "darkslategray3", alpha = 0.5) +
  theme_classic() +
  labs(y = "Edad") +
  scale_y_continuous(labels = scales::dollar) +
  scale_x_discrete( ) 

ggarrange(d1, d2, d3, d4, d5, nrow = 3, ncol = 2)
(d1 | d2) /
  (d3 | d4)


##Distribución Educación
d5 <- ggplot(geih_clean, aes(as.factor(p6210), totalHoursWorked)) +
  geom_boxplot(fill = "darkslategray3", alpha = 0.5) +
  theme_classic() +
  labs(y = "Horas trabajadas") +
  scale_y_continuous() +
  scale_x_discrete( ) 
d5
##Distribución por ingreso y sexo
d6 <- ggplot(geih_clean, aes(as.factor(p6210), ingtot)) +
  geom_boxplot(fill = "darkslategray3", alpha = 0.5) +
  theme_classic() +
  labs(y = "Ingreso total") +
  scale_y_continuous() +
  scale_x_discrete( ) 

d7 <- ggplot(geih_clean, aes(as.factor(sex), ingtot)) +
  geom_boxplot(fill = "darkslategray3", alpha = 0.5) +
  theme_classic() +
  labs(y = "Ingreso total") +
  scale_y_continuous() +
  scale_x_discrete( ) 

(d7 | d6) /
  (d5)

##Estadísticas descriptivas, tablas y figuras

#Variables de interés defindas: 

##Las variables que se considera pueden influir en el salario es si se encuentran en una zona urbana-rural,
##el sexo, el número de horas trabajadas, el nivel máximo de educación alcanzado, el oficio y la experiencia en el oficio actual
##(clase (urbano-rural), maxEducLevel (sim. P6210),  oficio, sex, totalHoursWorked, P6426 (experiencia en oficio actual))

#Distribución de categóricas
##Distribución de nivel educativo
g1 <- ggplot() +
  geom_bar(data = geih_clean, aes(x = p6210), 
           fill = "darkslategray3", alpha = 0.5) +
  labs(x = "Nivel educativo", y = "Frequencia") + 
  theme_classic()

g2 <- ggplot() +
  geom_bar(data = geih_clean, aes(x = maxEducLevel), 
           fill = "darkslategray3", alpha = 0.5) +
  labs(x = "Nivel Educativo Máximo", y = "Frequencia") + 
  theme_classic()

##Distribución por sexo
g3 <- ggplot() +
  geom_bar(data = geih_clean, aes(x = sex), 
           fill = "darkslategray3", alpha = 0.5) +
  labs(x = "Sexo", y = "Frequencia") + 
  theme_classic()
(g1 | g2) /
  (g3)
##Distribución por zona urbana-rural
ggplot() +
  geom_bar(data = geih_clean, aes(x = clase), 
           fill = "darkslategray3", alpha = 0.5) +
  labs(x = "Zona", y = "Frequencia") + 
  theme_classic()
##Todas las observaciones se encuentran en zona urbana, por lo que no se tendrá en cuenta

by1 <- ggplot(data=geih_clean) +
  geom_histogram(mapping=aes(x=ingtot , group= as.factor(sex), fill=as.factor(sex))) + 
  theme_classic()
by1 + scale_fill_manual(values =c("0"="darkslategray3", "1"="blue"), label=c("0"="Mujer", "1"="Hombre"), name="Sexo")

by2 <- ggplot(data=geih_clean) +
  geom_histogram(mapping=aes(x=ingtot , group= as.factor(p6210), fill=as.factor(p6210))) + 
  theme_classic()
by2 + scale_fill_manual(values =c("1"="darkslategray3", "2"="blue", "3"="aquamarine4", "4"="azure3", "5"="chartreuse4", "6"="chocolate3", "9"="coral2"), 
                        label=c("1"="Ninguno", "2"="Preescolar", "3"="Básica Primaria", "4"="Básica Secundaria", "5"="Media", "6"="Superior o universitaria", "9"="No sabe"), name="Nivel Educativo")


##Correlación ingresos y horas trabajadas
cor1 <- ggplot(geih_clean, aes(x = totalHoursWorked, y = ingtot)) +
  geom_point(color = "darkslategray3", alpha = 0.5) +
  theme_classic() +
  scale_y_continuous(labels = scales::dollar) +
  # scale_y_continuous(labels = scales::dollar, trans = 'log10') +
  labs(x = "Horas Trabajadas", y = "Ingresos totales (log)")
##Correlación ingresos y experiencia
cor2 <- ggplot(geih_clean, aes(x = p6426, y = ingtot)) +
  geom_point(color = "darkslategray3", alpha = 0.5) +
  theme_classic() +
  scale_y_continuous(labels = scales::dollar) +
  # scale_y_continuous(labels = scales::dollar, trans = 'log10') +
  labs(x = "Experiencia", y = "Ingresos totales (log)")

##Correlación ingresos y edad
cor3 <- ggplot(geih_clean, aes(x = age, y = ingtot)) +
  geom_point(color = "darkslategray3", alpha = 0.5) +
  theme_classic() +
  scale_y_continuous(labels = scales::dollar) +
  # scale_y_continuous(labels = scales::dollar, trans = 'log10') +
  labs(x = "Edad", y = "Ingresos totales (log)")

(cor1 | cor2) /
  (cor3)

#Tablas
##Se etiquetarán las variables para mayor claridad
geih_clean = apply_labels(geih_clean,
                          ingtot = "Ingreso Total",
                          totalHoursWorked = "Horas trabajadas",
                          age = "Edad",
                          p6426 = "Experiencia",
                          p6210 = "Nivel Educativo",
                          sex = "Sexo")
#Tablas descriptivas

vartable <- st(geih_clean, vars = c('ingtot','totalHoursWorked', 'p6426', 'age'), labels = c("Ingresos Totales", "Horas trabajadas", "Experiencia", "Edad"))

sumar1 <- describe(geih_clean[ , c('ingtot', 'age', 'totalHoursWorked', 'p6426')],fast=TRUE)

stargazer(geih_clean[c('ingtot', 'age', 'totalHoursWorked', 'p6426')], type = "text", 
          digits=2, median = TRUE, iqr = TRUE,
          title="Descriptive statistics",
          covariate.labels=c("Ingreso total","Edad","Horas trabajadas","p6426"),out="table1.txt")

##Correlaciones
# Primero seleccionamos las columnas numéricas
#Se obtienen las correlaciones
correla <- geih_clean[c('ingtot', 'age', 'totalHoursWorked', 'p6426')]
mcor <- round(cor(correla[, unlist(lapply(correla, is.numeric))]),2)
#Se mantiene toda la tabla
upper<-mcor
upper[upper.tri(mcor)]<-""
upper<-as.data.frame(upper)

corrplot(cor(correla[, unlist(lapply(correla, is.numeric))]))
write_xlsx(upper,"Correla.xlsx")

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#%    Age-earnings profile 		          %
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

##El ingreso total observado es  la suma de los ingresos percibidos en las siguientes fuentes: 
#ingreso monetario primera actividad (impa), ingreso segunda actividad (isa), ingreso en especie (ie), 
#ingreso monetario desocupados e inactivos (imdi) e ingresos provenientes de otras fuentes no laborales (iof) (intereses, pensiones, ayudas,
#cesantias, arriendos y otros) .

#El ingreso total imputado es la suma de cada una de las fuentes de ingresos 
#imputadas a los registros faltantes

#El ingreso total es la suma de cada una de las fuentes de ingresos
#tanto observadas como imputadas.
#Se analizan las distribuciones de los dos tipos de ingreso, dado que ingtotes se eliminó por la cantidad de missings

i1 <- ggplot(geih_clean, aes(y = ingtot)) +
  geom_boxplot(fill = "darkslategray3", alpha = 0.5) +
  theme_classic() +
  labs(y = "Ingresos totales") +
  scale_y_continuous(labels = scales::dollar) +
  scale_x_discrete( ) 

i2 <- ggplot(geih_clean, aes(y = ingtotob)) +
  geom_boxplot(fill = "darkslategray3", alpha = 0.5) +
  theme_classic() +
  labs(y = "Ingresos totales observados") +
  scale_y_continuous(labels = scales::dollar) +
  scale_x_discrete( ) 

(i1 | i2) 

ggplot(geih_clean, aes(x = ingtotob, y = ingtot)) +
  geom_point(color = "darkslategray3", alpha = 0.5) +
  theme_classic() +
  scale_y_continuous(labels = scales::dollar) +
  scale_x_continuous(labels = scales::dollar) +
  # scale_y_continuous(labels = scales::dollar, trans = 'log10') +
  labs(x = "Ingreso observado", y = "Ingresos totales (log)")

##Correr modelo OLS
geih_clean <- geih_clean %>%
  mutate(geih_clean, age2=age^2)
modelOLS <- lm(ingtot ~ age+age2, data = geih_clean)
summary(modelOLS)
require("stargazer")
stargazer(modelOLS)

##R squared del 1.6%

length(predict(modelOLS)) == length(geih_clean$ingtot)
plot(predict(modelOLS), geih_clean$ingtot[1:length(predict(modelOLS))])
abline(a=0, b=1)

##Se limitan las observaciones para una mejor observación de la estimación predicha vs la observada
plot(x = predict(modelOLS), y = geih_clean$ingtot,
     xlab='Predicted Values',
     ylab='Actual Values',
     main='Predicted vs. Actual Values', 
     ylim = c(0, 5000000))
abline(a=0, b=1)

plot(y = predict(modelOLS), x = geih_clean$age,
     ylab='Predicted Values',
     xlab='Edad',
     main='Predicted Income by age')
abline(a=0, b=1)

##What is the “peak age” suggested by the above equation? Use bootstrap to calculate the standard errors and construct the confidence intervals.

require("tidyverse")
set.seed(1111)
r = round(nrow(geih_clean)*0.3, 0)
R<-r # Number of Repetions
eta_mod1<-rep(0,R)

for (i in 1:R){
  geih_sample<- sample_frac(geih_clean,size=1,replace=TRUE)
  f <-lm(ingtot ~ age+age2, geih_sample)
  coefs<-f$coefficients
  eta_mod1[i]<-coefs[2]
}  
plot(hist(eta_mod1), main='Histograma Bootstrap')
mean(eta_mod1)
sqrt(var(eta_mod1))
quantile(eta_mod1,c(0.025,0.975))

##O con el comando boot 

require("boot")
Predictvalues <- function(geih_clean, index) {
  coef(lm(ingtot ~ age+age2, data = geih_clean, subset = index))
  
} 

boot(geih_clean, Predictvalues, R = r)

##Se saca la derivada e iguala a cero para obtener la "peak age"

beta <- coef(lm(ingtot ~ age+age2, data = geih_clean))
beta1 <- beta[1]
beta2 <- beta[2]
beta3 <- beta[3]

funcion <- expression(beta1+beta2*x+beta3*x^2)

derivada <- D(funcion,'x')
peak_age <- (beta2/(-beta3*2))
peak_age

#-----------Punto 4 GAP Earnings GAP-------------------
#-----------Punto 4-------------------
#Base a utilizar en este punto
geih_punto4 <- geih_clean %>%
  select(ingtot, sex, age)

#Transformar el ingreso en logaritmo
geih_punto4 <- geih_punto4 %>%
  mutate(logIng = log(ingtot))

geih_punto4 <- geih_punto4 %>%
  mutate(logIng = ifelse(logIng < 0, 0, logIng))

#Mirar las proporciones
table(geih_punto4$sex)


#Los 1 son hombres, hay que cambiar esto para hacer nuestro modelo
geih_punto4 <- geih_punto4 %>%
  mutate(female = ifelse(sex == "0", 1, 0))

geih_punto4 <- geih_punto4 %>%
  mutate(female = as.factor(female))

prop.table(table(geih_punto4$female))
#La muestra está balanceada 52% hombres 47%mujer


str(geih_punto4)
summary(geih_punto4)

#Correlación
M_p4 <- geih_punto4 %>%
  select(-sex) %>%
  mutate(female = as.numeric(female)) %>%
  cor()

M_p4 #No está tan clara la correlación
corrplot::corrplot(M_p4)

#Modelo lineal
#Estimar el modelo de gap
ModeloGap <- lm(logIng ~ female, data = geih_punto4)



geih_punto4 %>% ggplot(aes(x = logIng)) + geom_histogram() + labs(x = "Logaritmo del ingreso")

summary(ModeloGap)

#El modelo muestra que la variable independiente es significativa y muestra que al ser mujer se reduce el ingreso en en un 19.6% Sin embargo,
#el R2 es solamente del 1.2%, lo que significa que la variable de género solo explica un 1.2% la varianza del modelo.
stargazer(ModeloGap, type = "text")
stargazer(ModeloGap, type = "text", out = "ModeloGap.txt")


#Se incluyen los y estimados para comprarar el modelo
geih_punto4 <- geih_punto4 %>%
  mutate(fitValues = ModeloGap$fitted.values)

#Solo hay dos valores para los y estimados: uno  para mujer (13.97) y otro para hombre (14.07)
summary(geih_punto4)
exp(max(geih_punto4$fitValues))
exp(min(geih_punto4$fitValues))

#Analizamos algunas gráficas
ggplot(geih_punto4, aes(x = logIng, y = fitValues, colour = female)) + geom_point()          
ggplot(geih_punto4, aes(x = fitValues, y = logIng, colour = female)) + geom_point()
ggplot(geih_punto4, aes(x= logIng, fill = female)) + geom_histogram(position = "identity", alpha = 0.5)
ggplot(geih_punto4, aes(x= fitValues, fill = female)) + geom_histogram(position = "identity", alpha = 0.5)


#Modelo con edad y genero
geih_punto4 <- geih_punto4 %>%
  mutate(age2 = age^2)

ModeloGapAge <- lm(logIng ~ female + age + age2, data = geih_punto4)
summary(ModeloGapAge)
stargazer(ModeloGapAge, type = "text", out = "ModeloGapAge.txt")

#Plot con los datos reales. Muestra que si tienen un mismo intercepto pero los ingresos divergen a lo largo del tiempo.
#Los hombres crecen y las mujeres decrecen
ggplot(geih_punto4, aes(x = age, y = logIng, colour = female)) + geom_point(alpha = 0.15) + geom_smooth(method = 'lm') +
labs(x="Edad", y="Logaritmo del ingreso", colour = "Mujer")  

#Plot con los Y estimados. No hay intercepción, aunque muestran la misma pendiente a lo largo del gráfico. Llegan a un punto máximo
#Y empieza a decrecer
ggplot(geih_punto4, aes(x = age, y = ModeloGapAge$fitted.values, colour = female)) + geom_point(alpha = 0.15) + geom_smooth() +
  labs(x="Edad", y="Logaritmo del ingreso", colour = "Mujer")  


#Ahora se realiza la regresion con la interacción
geih_punto4$numfem <- as.numeric(geih_punto4$female)
geih_punto4$numfem <- ifelse(geih_punto4$numfem == 2, 1, 0)

geih_punto4$agefem <- geih_punto4$numfem*geih_punto4$age

#ExportarBD
write.table(geih_punto4, file = "Geih_punto4.csv", sep = ",", quote = FALSE, row.names = F)

#Los coeficientes cambian, hay un mayor impacto de género. EL R2 aumenta a 4,24%
ModeloGapAgeInt <- lm(logIng ~ female + age + age2 + agefem, data = geih_punto4)
coef(ModeloGapAgeInt)
summary(ModeloGapAgeInt)
stargazer(ModeloGapAgeInt, type = "text", out = "ModeloGapAgeInt.txt")
#Plot con los Y estimados. No hay intercepción, aunque muestran la misma pendiente a lo largo del gráfico. Llegan a un punto máximo
#Y empieza a decrecer
ggplot(geih_punto4, aes(x = age, y = ModeloGapAgeInt$fitted.values, colour = female)) + geom_point(alpha = 0.15) + geom_smooth() +
  labs(x="Edad", y="Logaritmo del ingreso", colour = "Mujer")  


#Peak Age con Bootstrap
p_load(boot)

model_coef <- function(data, index){
  coef(lm(logIng ~ female + age + age2, data = data, subset = index)) #Crear la función para calcular los errores estándar
}
model_coef(geih_punto4, 1:16138) #Probar la función

r = round(nrow(geih_punto4)*0.2, 0)
ModeloBoot_AgeCAP2 <- boot(geih_punto4, model_coef, R= r) #Correr el boot. Se puede ver que los errores estándar mejoraran con el boot

BootSE <- as.data.frame(ModeloBoot_AgeCAP2$t)

#Calculamos los intervalos de confianza para cada variable
hist(BootSE$V2)
IC_FMod1 <- quantile(BootSE$V2,c(0.025,0.975))

hist(BootSE$V3)
IC_AgeMod1 <- quantile(BootSE$V3,c(0.025,0.975))

hist(BootSE$V4)
IC_Age2Mod1 <- quantile(BootSE$V4,c(0.025,0.975))

#Ninguna de las variables tiene overlap en los intervalos

max(ModelosBoot_AgeCAP$finalModel$fitted.values)
max(ModeloGapAge$fitted.values)

geih_punto4$fitValuesGAPAGE <- ModelosBoot_AgeCAP$finalModel$fitted.values

#PeakAge
peaked_age <- ModeloBoot_AgeCAP2$t0[3]/(-ModeloBoot_AgeCAP2$t0[4]*2)
# El área bajo este modelo es de 44.2 años para los dos generos

#Bootstrap para interacción

model_coef2 <- function(data, index){
  coef(lm(logIng ~ female + age + age2 + agefem, data = data, subset = index)) #Crear la función para calcular los errores estándar
}
model_coef2(geih_punto4, 1:16138) #Probar la función

ModeloBoot_AgeFEM <- boot(geih_punto4, model_coef2, R=r) #Correr el boot. Se puede ver que los errores estándar mejoraran con el boot
summary(ModeloBoot_AgeFEM)


#Calculamos los intervalos de confianza para cada variable
BootSEFAGE <- as.data.frame(ModeloBoot_AgeFEM$t)

#Calculamos los intervalos de confianza para cada variable
hist(BootSEFAGE$V2)
IC_FMod2 <- quantile(BootSEFAGE$V2,c(0.025,0.975))

hist(BootSEFAGE$V3)
IC_AgeMod2 <- quantile(BootSEFAGE$V3,c(0.025,0.975))

hist(BootSEFAGE$V4)
IC_Age2Mod2 <- quantile(BootSEFAGE$V4,c(0.025,0.975))

hist(BootSEFAGE$V5)
IC_AgeFemMod2 <- quantile(BootSEFAGE$V5,c(0.025,0.975))

IntervaloDeConfianza <- c(IC_FMod2, IC_AgeMod2, IC_Age2Mod2, IC_AgeFemMod2)
NamesIC <- c("IC_FMod2", "IC_AgeMod2", "IC_Age2Mod2", "IC_AgeFemMod2")
IntervaloDeConfianza <- matrix(IntervaloDeConfianza, nrow = 4, ncol = 2)
rownames(IntervaloDeConfianza) <- NamesIC
colnames(IntervaloDeConfianza) <- c("2.5%", "97.5%")
stargazer(IntervaloDeConfianza, type ="text", out = "IC_Punto4.txt")

#peak age
PeakAgeMujer <- (ModeloBoot_AgeFEM$t0[3] + ModeloBoot_AgeFEM$t0[5])/(-2*ModeloBoot_AgeFEM$t0[4])
#Las mujeres tienen su cúspide salarial a los 39.31 años

PeakAgeHombre <- ModeloBoot_AgeFEM$t0[3]/(-2*ModeloBoot_AgeFEM$t0[4])
#Los hombres, por su parte tienen su cúspide salarial a los 48.25 años

#Punto 4 - Teorema FWL
#se incorpora la variable oficio en la base de datos 
str(geih_clean$oficio)
geih_punto4$oficio = geih_clean$oficio
require("tidyverse")
#Se analiza la relación entre las variables graficamente 
ggplot(geih_punto4) +
  geom_bar(aes(x=female, fill=logIng))
ggplot(geih_punto4) +
  geom_bar(aes(x=oficio, fill=logIng))
ggplot(geih_punto4) +
  geom_histogram(aes(x=logIng, colour = female)) + facet_wrap(~oficio)
#Se incorpora en el modelo la variable oficio 
modelo.ols.ep<-lm(logIng ~ female + oficio, data = geih_punto4)
summary(modelo.ols.ep)
stargazer(modelo.ols.ep, type="text") #En el primer modelo - coeficiente es -0.18*** y r2 es 0.185
#Calculamos female tilde (y=female x=oficio)
modelo_female_tilde<-lm(as.numeric(female) ~ oficio, data=geih_punto4)
summary(modelo_female_tilde)
female_tilde<-modelo_female_tilde$residuals
reg2_ep<-lm(logIng ~ female_tilde, data = geih_punto4)
summary(reg2_ep)
stargazer(reg2_ep, type="text") #En esta regresion se observa que el coeficiente sigue siendo -0.18
#Calculamos ingreso tilde (y=ingreso x=oficio)
modelo_ingreso_tilde<-lm(logIng~ oficio, data=geih_punto4)
ingreso_tilde<-modelo_ingreso_tilde$residuals
reg3_ep<-lm(ingreso_tilde~ female_tilde)
summary(reg3_ep)
stargazer(modelo.ols.ep,reg2_ep,reg3_ep, type="text" , out = "TeoremaFWL.txt") 
#Finalmente, se observa que en las tres regresiones el coeficiente es igual

#--------------Punto 5----------------------------------------------

#a. Split the sample

#Incluir variables de los modelos anteriores
geih_clean$logIng <- log(geih_clean$ingtot)
geih_clean$logIng <- ifelse(geih_clean$logIng < 0, 0, geih_clean$logIng)
geih_clean$age2 <- (geih_clean$age)^2
geih_clean$female <- ifelse(geih_clean$sex == "0",1,0)
geih_clean$agefem <- geih_clean$age*geih_clean$female
geih_clean$totalHoursWorked2<-(geih_clean$totalHoursWorked)^2
geih_clean$totalHoursWorked3<-(geih_clean$totalHoursWorked)^3

#Separar la muestra:
set.seed(10101) #Sembrar la semilla para aleatorizar la muestra
#Dividir la muestra en 70% para entrenar y 30% para hacer el testeo
index <-  round(nrow(geih_clean)*0.3,digits=0)
#Muestra aleatorizada del df y mantener el número de observaciones del indice
test.indices <- sample(1:nrow(geih_clean), index)
# set de entrenamiento
geih.train<-geih_clean[-test.indices,] 
#30% set de testeo
geih.test<-geih_clean[test.indices,] 
#Seleccionar el set de entrenamiento en variables independientes y dependientes
YTrain <- geih.train$ingtot
XTrain <- geih.train %>% select(-ingtot)
#Seleccionar el set de testeo en variables independientes y dependientes
YTest <- geih.test$ingtot
XTest <- geih.test %>% select(-ingtot)

#Regresion de solo el intercepto
Modelobase <- lm(ingtot ~ 1, data = geih.train)
stargazer(Modelobase, type ="text")
mean(geih.train$ingtot)

#El intercepto es la media de los ingresos totales de las personas
#Ahora estimamos nuestros modelos anteriores
#Modelo Edad
ModeloEdad <- lm(ingtot ~ age + age2, data = geih.train)

#Modelo GAP y edad
ModeloFemale <- lm(logIng ~ female, data = geih.train)
ModeloAgeGAP <- lm(logIng ~ female + age + age2 + agefem, data = geih.train)

stargazer(Modelobase, ModeloEdad, ModeloFemale, ModeloAgeGAP, type = "text")

#Modelos con transformaciones en x - Mayor complejidad
#Modelo 1. 
Modelo_C_1<-lm(logIng ~ female + estrato1 + p6426 + maxEducLevel, data=geih.train)
#Modelo 2. 
Modelo_C_2<-lm(logIng ~ female + p6426 + maxEducLevel + female*maxEducLevel, data=geih.train)
#Modelo 3. 
Modelo_C_3<-lm(logIng ~ female + p6426 + age + age2 + p6426*age, data=geih.train)
#Modelo 4. 
Modelo_C_4<-lm(logIng ~ female + p6426 + informal + p6240 + p7090 + totalHoursWorked2 + totalHoursWorked3, data=geih.train)
#Modelo 5
Modelo_C_5<-lm(logIng ~ female + p6426 + totalHoursWorked + age + totalHoursWorked*female + p7090*female, data=geih.train)


#Reporte y comparacion de prediction errors
logYtest <- log(YTest)
logYtest <- ifelse(logYtest < 0 , 0, logYtest)

#RMSE Modelo 1
Modelo_C_1_Summ<-summary(Modelo_C_1)
rmse_1_dentromuestra<-sqrt(mean(Modelo_C_1_Summ$residuals^2))
print(rmse_1_dentromuestra) #RMSE1 = 1.047089
YhatModelo1 <- predict(Modelo_C_1, XTest)
SSEModelo1 <- sum((YhatModelo1-logYtest)^2)
MSE_1_fueramuestra <- SSEModelo1/length(logYtest)
RMSE_1_fueramuestra <- sqrt(MSE_1_fueramuestra)

#RMSE Modelo 2
Modelo_C_2_Summ<-summary(Modelo_C_2)
rmse_2<-sqrt(mean(Modelo_C_2_Summ$residuals^2))
print(rmse_2) #RMSE2 = 1.082645
YhatModelo2 <- predict(Modelo_C_2, XTest)
SSEModelo2 <- sum((YhatModelo2-logYtest)^2)
MSE_2_fueramuestra <- SSEModelo2/length(logYtest)
RMSE_2_fueramuestra <- sqrt(MSE_2_fueramuestra)

#RMSE Modelo 3
Modelo_C_3_Summ<-summary(Modelo_C_3)
rmse_3<-sqrt(mean(Modelo_C_3_Summ$residuals^2))
print(rmse_3) #RMSE3 = 1.146387
YhatModelo3 <- predict(Modelo_C_3, XTest)
SSEModelo3 <- sum((YhatModelo3-logYtest)^2)
MSE_3_fueramuestra <- SSEModelo3/length(logYtest)
RMSE_3_fueramuestra <- sqrt(MSE_3_fueramuestra)

#RMSE Modelo 4
Modelo_C_4_Summ<-summary(Modelo_C_4)
rmse_4<-sqrt(mean(Modelo_C_4$residuals^2))
print(rmse_4) #RMSE4 =  1.042958
YhatModelo4 <- predict(Modelo_C_4, XTest)
SSEModelo4 <- sum((YhatModelo4-logYtest)^2)
MSE_4_fueramuestra <- SSEModelo4/length(logYtest)
RMSE_4_fueramuestra <- sqrt(MSE_4_fueramuestra)


#RMSE Modelo 5 
Modelo_C_5_Summ<-summary(Modelo_C_5)
rmse_5<-sqrt(mean(Modelo_C_5_Summ$residuals^2))
print(rmse_5) #RMSE3 = 1.132367
YhatModelo5 <- predict(Modelo_C_5, XTest)
SSEModelo5 <- sum((YhatModelo5-logYtest)^2)
MSE_5_fueramuestra <- SSEModelo5/length(logYtest)
RMSE_5_fueramuestra <- sqrt(MSE_5_fueramuestra)

RMSE_DentroMuestra <- c(rmse_1_dentromuestra, rmse_2, rmse_3, rmse_4, rmse_5)
RMSE_FueraMuestra <- c(RMSE_1_fueramuestra, RMSE_2_fueramuestra, RMSE_3_fueramuestra, RMSE_4_fueramuestra, RMSE_5_fueramuestra)

ErroresModelos <- data.frame(RMSE_DentroMuestra, RMSE_FueraMuestra)
NombresModelos <- c("Modelos_C_1", "Modelos_C_2", "Modelos_C_3", "Modelos_C_4", "Modelos_C_5")
ErroresModelos <- as.matrix(ErroresModelos)
rownames(ErroresModelos) <- NombresModelos
write.table(ErroresModelos, file = "ErroresModelos.csv", sep = ",", quote = FALSE, row.names = T)

#El modelo 4 es el que tiene el menor RMSE. Utilizamos la variable P6426 como experiencia de la persona, p6240 para ver qué actividad estaba haciendo la
#semana pasada y si eso influye en el ingreso, p7090 para ver si la persona quiere trabajar más horas y se incluyen variables polinómicas de grado 2 y 3 del
#numero de horas trabajadas

#v. Leverage Statistic
#Usar solo las variables a usas
p_load(matlib)
#Solo se seleccionan las variables que se usan en el modelo
LeverageMatrix <- select(geih.test, female, p6426, informal, p6240, p7090, totalHoursWorked2, totalHoursWorked3)
str(LeverageMatrix)
#Es necesario modificar el df ya que no se pueden tener factores en la matrix
#Se crean dummies para cada caso
LeverageMatrix$informal <- ifelse(LeverageMatrix$informal == "1", 1, 0)
LeverageMatrix$p7090 <- ifelse(LeverageMatrix$p7090 == "2", 1, 0)
LeverageMatrix$p62401 <- ifelse(LeverageMatrix$p6240 == "1", 1, 0)
LeverageMatrix$p62402 <- ifelse(LeverageMatrix$p6240 == "2", 1, 0)
LeverageMatrix$p62403 <- ifelse(LeverageMatrix$p6240 == "3", 1, 0)
LeverageMatrix$p62404 <- ifelse(LeverageMatrix$p6240 == "4", 1, 0)
LeverageMatrix$p62406 <- ifelse(LeverageMatrix$p6240 == "6", 1, 0)
LeverageMatrix <- select(LeverageMatrix, -p6240)
str(LeverageMatrix)
#Se vuelve matrix
LeverageMatrix <- data.matrix(LeverageMatrix)
str(LeverageMatrix)
LeverageMatrix < as.numeric(LeverageMatrix)
#Se crea la transpuesta
TLevMatriz <- t(LeverageMatrix)
#Multiplicacion de la matriz con su transpuesta
Lev_por_Trans <- TLevMatriz%*%LeverageMatrix
#La inversa
Inv_Lev_por_Trans <- inv(TLevMatriz%*%LeverageMatrix)
#La Matriz H
HMatrix <- LeverageMatrix%*%Inv_Lev_por_Trans%*%TLevMatriz
#La diagonal de la matriz H
h_j <- diag(HMatrix)
#Residuales
red_j <- (logYtest - predict(Modelo_C_4, XTest))
#EL Alpha
Alpha_j <- red_j/(1-h_j)
#Yhat
Yh_j <- predict(Modelo_C_4, XTest)
#Introducimos las estadísticas al data fram
LeverageStatistics <- select(geih.test, female, p6426, informal, p6240, p7090, totalHoursWorked2, totalHoursWorked3, logIng)
df_statistics <- data.frame(red_j, h_j, Alpha_j, Yh_j)
LeverageStatistics <- cbind(LeverageStatistics, df_statistics)

ggplot(LeverageStatistics, aes(x=h_j)) + geom_boxplot()
ggplot(LeverageStatistics, aes(x=Alpha_j)) + geom_boxplot()
ggplot(LeverageStatistics, aes(x=Alpha_j)) + geom_histogram()
ggplot(LeverageStatistics, aes(x=red_j)) + geom_histogram(aes(x = Alpha_j))

#Los alphas son casi iguales a los residuales, por ende, no hay un high leverage en este modelo, tal vez esto pase por error en la construccións del modelo
#Sin embargo, es interesante discutir que las personas con un hat mayor son aquellas con mayor edad.

#B. K-fold Cross Validation

#Modelo 1
set.seed(10101)
trControl_1 <- trainControl(method  = "cv",
                          number  = 10) #funcion para realizar la cross validation dentro de train dividir en 10 folds

Modelo1_cv <- train(logIng ~ female + estrato1 + p6426 + maxEducLevel,
                    data = geih.train,
                    trControl = trControl_1,
                    method = "lm")
Modelo1_cv
Modelo1_cv_Yhat <- predict(Modelo1_cv, XTest)
RMSE_Modelo1_cv <- sqrt(sum((Modelo1_cv_Yhat-logYtest)^2)/length(logYtest))

#Modelo 2
set.seed(10101)
trControl_2 <- trainControl(method  = "cv",
                            number  = 10) #funcion para realizar la cross validation dentro de train dividir en 10 folds

Modelo2_cv <- train(logIng ~ female + p6426 + maxEducLevel + female*maxEducLevel,
                    data = geih.train,
                    trControl = trControl_2,
                    method = "lm")
Modelo2_cv
Modelo2_cv_Yhat <- predict(Modelo2_cv, XTest)
RMSE_Modelo2_cv <- sqrt(sum((Modelo2_cv_Yhat-logYtest)^2)/length(logYtest))

#Modelo 3
set.seed(10101)
trControl_3 <- trainControl(method  = "cv",
                            number  = 10) #funcion para realizar la cross validation dentro de train dividir en 10 folds

Modelo3_cv <- train(logIng ~ female + p6426 + age + age2 + p6426*age,
                    data = geih.train,
                    trControl = trControl_3,
                    method = "lm")
Modelo3_cv
Modelo3_cv_Yhat <- predict(Modelo3_cv, XTest)
RMSE_Modelo3_cv <- sqrt(sum((Modelo3_cv_Yhat-logYtest)^2)/length(logYtest))

#Modelo 4
set.seed(10101)
trControl_4 <- trainControl(method  = "cv",
                            number  = 10) #funcion para realizar la cross validation dentro de train dividir en 10 folds

Modelo4_cv <- train(logIng ~ female + p6426 + informal + p6240 + p7090 + totalHoursWorked2 + totalHoursWorked3,
                    data = geih.train,
                    trControl = trControl_4,
                    method = "lm")
Modelo4_cv
Modelo4_cv_Yhat <- predict(Modelo4_cv, XTest)
RMSE_Modelo4_cv <- sqrt(sum((Modelo4_cv_Yhat-logYtest)^2)/length(logYtest))

#Modelo 5
set.seed(10101)
trControl_5 <- trainControl(method  = "cv",
                            number  = 10) #funcion para realizar la cross validation dentro de train dividir en 10 folds

Modelo5_cv <- train(logIng ~ female + p6426 + totalHoursWorked + age + totalHoursWorked*female + p7090*female,
                    data = geih.train,
                    trControl = trControl_5,
                    method = "lm")
Modelo5_cv
Modelo5_cv_Yhat <- predict(Modelo5_cv, XTest)
RMSE_Modelo5_cv <- sqrt(sum((Modelo5_cv_Yhat-logYtest)^2)/length(logYtest))

#Sigue siendo mejor el modelo 4
RMSE_CVs <- c(RMSE_Modelo1_cv, RMSE_Modelo2_cv, RMSE_Modelo3_cv, RMSE_Modelo4_cv, RMSE_Modelo5_cv)

#Revisamos los alphas del Modelo 4:
Resid_j_CV <- (logYtest - Modelo4_cv_Yhat)
Alpha_j_CV <- (Resid_j_CV)/(1-h_j)
Df_Cv_M4 <- data.frame(logYtest, Modelo4_cv_Yhat, h_j, Resid_j_CV, Alpha_j_CV)
#Mismo comportamiento, las observaciones con altos h_j no influyen en la muestra


#5.c. LOOCV
dfLOOCV <- data.frame() #Creamos el df del for
SSELOOCV <- 0
MSELOOCV <- 0
logYtrain <- log(YTrain)
logYtrain <- ifelse(logYtrain < 0, 0, logYtrain)

for (i in 1:nrow(geih.train)) {
  
  dfLOOCV <- geih.train[-i,] #El df del for va a ser sin la i-esima observacion
  ModeloLOOCV <- lm(logIng ~ female + p6426 + informal + p6240 + p7090 + totalHoursWorked2 + totalHoursWorked3, data = dfLOOCV) #Creo el modelo sin esa observación
  Yhat_LOOCV <- predict(ModeloLOOCV, XTrain[i,]) #Testeo el modelo
  Res_LOOCV <- logYtrain[i] - Yhat_LOOCV #saco los residuales
  Res_LOOCV2 <- Res_LOOCV^2 #Residuales al cuadrado
  SSELOOCV <- SSELOOCV + Res_LOOCV2 #Sumo los residuales al cuadrado
  MSELOOCV <- SSELOOCV/i #MSE
  print(i)
  print(MSELOOCV)
}

MSELOOCV
RMSELOOCV <- sqrt(MSELOOCV)

#Los errores disminuyen con este métedo, pero es muy costoso computacionalmente
RMSEM4 <- c(RMSE_4_fueramuestra, RMSE_Modelo4_cv, RMSELOOCV)

#Probemos con Caret
ctrl <- trainControl(method = "LOOCV")
ModeloLOOCV2 <- train(logIng ~ female + p6426 + informal + p6240 + p7090 + totalHoursWorked2 + totalHoursWorked3, 
                      data = geih.train,
                      method = "lm",
                      trControl = ctrl)
ModeloLOOCV2
Yhat_LOOCV2 <- predict(ModeloLOOCV2, XTest)
RMSE_LOOCV2_Fueramuestra <- sqrt(sum((logYtest-Yhat_LOOCV2)^2)/length(logYtest))
Resid_j_LOOCV2 <- (logYtest - Modelo4_cv_Yhat)
Alpha_j_LOOCV2 <- (Resid_j_CV)/(1-h_j)
Df_LOOCV2 <- data.frame(logYtest, Yhat_LOOCV2, h_j, Resid_j_LOOCV2, Alpha_j_LOOCV2)

head(Df_LOOCV2)
p_load(psych)

Tabla_5C <- describe(Df_LOOCV2)
write.table(Tabla_5C, file = "Tabla_5c.csv", sep = ",", quote = FALSE, row.names = T)
Tabla_5B <- describe(Df_Cv_M4)
write.table(Tabla_5B, file = "Tabla_5b.csv", sep = ",", quote = FALSE, row.names = T)
Tabla_5A <- describe(LeverageStatistics)
write.table(Tabla_5A, file = "Tabla_5a.csv", sep = ",", quote = FALSE, row.names = T)

write.table(geih.train, file = "geihtrain.csv", sep = ",", quote = FALSE, row.names = T)
write.table(geih.test, file = "geihtest.csv", sep = ",", quote = FALSE, row.names = T)
#Con este código el RMSE disminuye un poco más pero sigue siendo igual de costoso computacionalmente
