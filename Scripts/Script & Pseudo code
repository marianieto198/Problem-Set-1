
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#%    Data Acquisition    		          %
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
##Paquetes 
rm(list = ls())
install.packages("rvest")
install.packages("pacman")
install.packages("ggpubr")
install.packages("devtools")
devtools::install_github("thomasp85/patchwork")

library(ggplot2)
library(patchwork)
p_load(rio) 
p_load(tidyverse)
p_load(e1071) 
p_load(EnvStats) 
p_load(tidymodels) 
p_load(ggplot2) 
p_load(scales) 
p_load(ggpubr) 
p_load(knitr) 
p_load(kableExtra)
p_load(ggplot2)



require(pacman)
library(rvest)
library(pacman)
library(dplyr)
library(ggplot2)
library(ggpubr)



#1. En un primer momento se explora el link para verificar la manera en la que está presentada la información
#En este caso, la muestra de GEIH se presenta en una tabla fija, por lo que es más fácil realizar el scrape.

#2. Ahora, con el uso del click derecho se inspecciona la página y se obtiene el url de acceso para el código html desde la ventana de network
#Se realiza un loop para cada chunk de valores

#3. Se define la base vacía o la base inicial que será llenada por la información scrapeada de los url
Pagina1 <- read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_1.html") %>%
                    html_table()  
class(Pagina1) #La variable es tipo List y toca volverla Data Frame

Pagina1 <- as.data.frame(Pagina1) Volvemos la lista un Data Frame
df <- Pagina1 #Convertimos la pagina 1 en el DF que se usará para el taller

#3.1.Para cada chunk de información se extrae un temporal en donde se irá almacenando la información de cada chunk. 
#Para este paso se utiliza el comando “for” junto con el comando “paste0” para concatenar los diferentes url en un único elemento. 
for (i in 2:10) {
   url <- paste0("https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_",i,".html") #Vamos iterando la pagina con i
  chunk_i <- read_html(url)  %>% 
    html_table()  # Se crea la lista del chunk
  chunk_i <- as.data.frame(chunk_i) #se vuelve data frame el chunk
#3.2.Se utiliza el comando rbind para pegar las filas de información almacenadas en cada archivo temporal y unificarlas en un único elemento.
 df <-  rbind(df, chunk_i) 
}

geih <- df # le cambiamos el nombre
###Otra opción es la siguiente:

#1.
#Crear una lista vacia en donde se van a almacenar las tablas tomadas del sitio web.
data=list()
#Loop para importar las 10 tablas y añadir a la lista data
for (i in 1:10){
  url=paste0('https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_',i,'.html')
  temp=read_html(url) %>%
    html_table()
  data=append(data,temp)
}

#Loop para concatenar las bases de datos y formar GEIH
geih=data.frame()
for (i in 1:10){
  geih=rbind(geih,data[[i]])
}

#Se evidencia que no hay restricciones para acceder a los datos y realizar el scrapping, lo anterior por lo que se
#mencionó con anterioridad de que al ser chunks de información fijos facilita la lectura y extracción de los datos.

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#%    Data Cleaning   		          %
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

##Ya al tener lista la base de datos GEIH contamos con 178 variables y 32177 observaciones.
##Ahora bien, se empezará analizando las dimensiones de la base 
dim(geih)
str(geih)
##Ahora se corrigen las variables para que sean leídas como categóricas, bulein o string dependiendo del caso. 
##Lo anterior, ya que en su mayoría fueron leídas como enteros.
##Se definen entonces las variables categóricas

geih <- geih %>%
  mutate_at(.vars = c(
    "cclasnr11", "cclasnr2", "cclasnr3", "cclasnr4", "cclasnr5",
    "cclasnr6", "cclasnr7", "cclasnr8", "clase", "college",
    "cotPension", "cuentaPropia", "depto", "directorio", "dominio",
    "dsi", "estrato1", "formal", "ina", "inac", "informal",
    "maxEducLevel", "p6050", "microEmpresa", "ocu", "oficio", 
    "orden", "p6090", "p6100", "p6210", "p6210s1", "p6240", "p6510",
    "p6510s2", "p6545", "p6545s2", "p6580", "p6580s2", "p6585s1",
    "p6585s1a2", "p6585s2", "p6585s2a2", "p6585s4", "p6585s4a2",
    "p6590", "p6610", "p6620", "p6630s1", "p6630s2", "p6630s3",
    "p6630s4", "p6630s6", "p6920", "p7040", "p7050", "p7090",
    "p7110", "p7120", "p7140s1", "p7140s2", "p7150", "p7160",
    "p7310", "p7350", "p7422", "p7472", "p7495", "p7500s1",
    "p7500s2", "p7500s3", "p7505", "p7510s1", "p7510s2",
    "p7510s3", "p7510s5", "p7510s6", "p7510s7", "pea", "pet", 
    "regSalud", "relab", "secuencia_p", "sex", "sizeFirm", "wap"),
    .funs = factor)

str(geih)
summary (geih) # se puede ver que hay un gran número de observaciones vacias

#Se elimina la primera variable que no es necesaria
geih <- geih %>%
      select(-Var.1)

#Ahora se necesita hacer el filtro de la base de datos por personas ocupadas, que vivan en Bogotá y que sean mayores de 18 años
geih <- geih %>%
      filter(ocu == 1,
             dominio == "BOGOTA",
             age > 18)

dim(geih) # La base pasó de 32177 observaciones a 16397 observaciones
summary(df) #se puede observar que aunque bajaron los NAs sigue habiendo una presencia significativa en muchas variables


##Tratamiento de missing values

cantidad_na <- sapply(geih, function(x) sum(is.na(x))) #Una función que me suma el número de NAs por variable
cantidad_na <- data.frame(cantidad_na) #Lo convierto en Data Frame
porcentaje_na <- cantidad_na/nrow(geih) #Le saco el porcentaje de Missing values a cada variable

# Porcentaje de observaciones faltantes. 
porcentaje <- mean(porcentaje_na[,1]) #El 45.21% de las variables tiene NAs
print(paste0("En promedio el ", round(porcentaje*100, 2), "% de las entradas están vacías"))

##Ordenamos de mayor a menor
porcentaje_na <- arrange(porcentaje_na, desc(cantidad_na))
# Convertimos el nombre de la fila en columna
porcentaje_na <- rownames_to_column(porcentaje_na, "variable")

# Quitamos las variables que no tienen NAs
filtro <- porcentaje_na$cantidad_na == 0
variables_sin_na <- porcentaje_na[filtro, "variable"]
str_count(variables_sin_na) #Hay 58 variables sin NA
variables_sin_na <- paste(variables_sin_na, collapse = ", ")
print(paste("Las variables sin NAs son:", variables_sin_na))

porcentaje_na <- porcentaje_na[!filtro,] #Quedan solo 119 variables con NAs

orden <- porcentaje_na$variable[length(porcentaje_na$variable):1] #Se vuelven caracteres
porcentaje_na$variable <- factor(porcentaje_na$variable,
                                 levels = orden) #Se utilizan como factores para poder graficar

str(porcentaje_na) # Se revisa el tipo de variables

# Como son tantas variables vamos a hacer una gráfica con los que tienen menos NAs
#para analizar si se pueden imputar los valores

ggplot(porcentaje_na[101:nrow(porcentaje_na),], 
       aes(y = variable, x = cantidad_na)) +
  geom_bar(stat = "identity", fill = "darkslategray3") +
  geom_text(aes(label = paste0(round(100*cantidad_na, 1), "%")),
            colour = "white", position = "dodge", hjust = 1.3,
            size = 2, fontface = "bold") +
  theme_classic() +
  labs(x = "Porcentaje de NAs", y = "Variables") +
  scale_x_continuous(labels = scales::percent, limits = c(0, 1))


Hay un salto de 36% de NAs de la variable ie a 10.6% de la variable  y_total_m 

#Si la cantidad de missing values es superior al 5% no se pueden imputar los datos (crearlos a partir de la media o la moda)
##Se eliminan las variables que tienen más del 5% de NAs
filtro2 <- porcentaje_na$cantidad_na > 0.05
variables_eliminadas <- porcentaje_na$variable[filtro2]
geih_clean <- geih %>%
  select(-variables_eliminadas) 
k0 <- ncol(geih)
k1 <- ncol(geih_clean)
print(paste("Se eliminaron", k0-k1, "variables. Ahora la base tiene", k1, "columnas."))
#Ahora solo tenemos 62 variables

porcentaje_na %>%
              filter(cantidad_na <= 0.05)
#Nos quedan 4 variables con NAs: impa, p7070, isa, maxEducLevel
 
#Como tres variables son continuas se reemplaza por la mediana, para que queden en el percentil 50% 
geih_clean <- geih_clean %>%
            mutate(impa = ifelse(is.na(impa), median(impa, na.rm = T), impa), #Mediana sin tener en cuenta los NAs
                   p7070 = ifelse(is.na(p7070), median(p7070, na.rm = T), p7070),
                   isa = ifelse(is.na(isa), median(isa, na.rm = T), isa))


sum(is.na(geih_clean$impa)) #Reviso
sum(is.na(geih_clean$p7070)) #Reviso
sum(is.na(geih_clean$isa)) #Reviso

#Para MaxLevelEduc toca sacar la moda y eso lo hacemos mirando su plot y después reemplazando
ggplot(geih_clean, aes(x = maxEducLevel)) + geom_bar()

#La moda es la categoría 7
moda_MaxEduc <- which(table(geih_clean$maxEducLevel) == max(table(geih_clean$maxEducLevel))) #Guardo el mayor valor
filtro2 <- is.na(geih_clean$maxEducLevel) #filtro solo los NAs
geih_clean$maxEducLevel[filtro2] <- moda_MaxEduc #Reemplazo los NAs por la moda
table(geih_clean$maxEducLevel) #Reviso

sum(is.na(geih_clean$maxEducLevel)) #Reviso

geih_clean %>% #Se revisa que todo el DF esté sin NAs
          is.na() %>%
          sum()



##Existe la opción de normalizar los datos, se agregará el código de manera demostrativa
#Sin embargo, no se nirmalizarán los datos para facilitar su interpretación.

# Primero seleccionamos las columnas numéricas
#filtro <- sapply(geih, is.numeric)
#geih_escalada <- geih
#geih_escalada[,filtro] <- scale(geih[,filtro])

# Podemos obtener las medias y las desviaciones con las que se hizo la transformación
#lista <- attributes(geih_escalada)
#medias <- lista$`scaled:center`
#desviaciones <-  lista$`scaled:scale`

# Visualicemos los resultados de media y desviación

# Media
#apply(geih_escalada[,filtro], MARGIN = 2, 
#     function(x) round(mean(x, na.rm = T), 2))

#apply(geih_escalada[,filtro], MARGIN = 2, 
#     function(x) round(sd(x, na.rm = T), 2))

#head(geih_escalada)


##Transformaciones para resolver asimetrías

# Primero seleccionamos las columnas numéricas
filtro <- sapply(geih_clean, is.numeric)
sk <- sapply(geih_clean[, filtro], skewness, na.rm = T)

# Vamos a buscar las variables con las mayores asimetrías sin importar la dirección
sk_abs <- abs(sk)
sk_abs <- data.frame(sk_abs)
sk_abs <- rownames_to_column(sk_abs, "variable")
sk_abs <- arrange(sk_abs, desc(sk_abs))

# Las variables con mayor asimetría son:
head(sk_abs)

head(sk_abs)

p_load(e1071)

hist(geih_clean$ingtot)

pload(e1071)

# Construimos el vector de los ingresos y eliminamos los NAs 
x <- geih_clean$ingtot[!is.na(geih$ingtot)]
x <- geih_clean$ingtot[geih_clean$ingtot!=0]
sk_x <- skewness(x)
print(paste("El valor de skewness para los ingresos totales es", round(sk_x, 2)))

x <- log(geih_clean$ingtot[!is.na(geih_clean$ingtot)])
x <- geih_clean$ingtot[geih_clean$ingtot!=0]
sk_x <- skewness(x)
print(paste("El valor de skewness para los ingresos totales es", round(sk_x, 2)))

# Vamos a aplicar diferentes transformaciones y a visualizar como cambia el skewness
# Encontrar lambda óptimo
lambda <- boxcox(x, objective.name = "Log-Likelihood", 
                 optimize = TRUE)$lambda
box_cox_x <- boxcoxTransform(x, lambda)

ing_tot <- data.frame("Ingresos totales" = x,
                      "Logaritmo" = log(x),
                      "Raiz cuadrada" = sqrt(x),
                      "Inversa" = 1/x,
                      "Box-Cox" = box_cox_x)


# Observemos la distribución original
p1 <- ggplot(ing_tot) +
  geom_histogram(aes(x = Ingresos.totales, 
                     fill = "Ingresos totales"), 
                 alpha = 0.5, fill = "gray", bins = 30) +
  geom_text(aes(x = Inf, y = Inf, hjust = 1, 
                vjust = 1, 
                label = paste("Skewness", round(sk_x, 2)))) +
  theme_classic() +
  labs(x = "Ingresos totales", y = "Cantidad") +
  scale_x_continuous(labels = scales::dollar)

sk_x2 <- skewness(ing_tot$Logaritmo)
p2 <- ggplot(ing_tot) +
  geom_histogram(aes(x = Logaritmo, 
                     fill = "Logaritmo"), 
                 alpha = 0.5, fill = "blue", bins = 30) +
  geom_text(aes(x = Inf, y = Inf, hjust = 1, 
                vjust = 1, 
                label = paste("Skewness", round(sk_x2, 2)))) +
  theme_classic() +
  labs(x = "Log(Ingresos totales)", y = "Cantidad") 

sk_x3 <- skewness(ing_tot$Raiz.cuadrada)
p3 <- ggplot(ing_tot) +
  geom_histogram(aes(x = Raiz.cuadrada, 
                     fill = "Raíz cuadrada"), 
                 alpha = 0.5, fill = "red", bins = 30) +
  geom_text(aes(x = Inf, y = Inf, hjust = 1, 
                vjust = 1, 
                label = paste("Skewness", round(sk_x3, 2)))) +
  theme_classic() +
  labs(x = "Raiz cuadrada de Ingresos totales", y = "Cantidad") 

sk_x4 <- skewness(ing_tot$Inversa)
p4 <- ggplot(ing_tot) +
  geom_histogram(aes(x = Inversa, 
                     fill = "Inversa"), 
                 alpha = 0.5, fill = "green", bins = 30) +
  geom_text(aes(x = Inf, y = Inf, hjust = 1, 
                vjust = 1, 
                label = paste("Skewness", round(sk_x4, 2)))) +
  theme_classic() +
  labs(x = "1/(Ingresos totales)", y = "Cantidad")

sk_x5 <- skewness(ing_tot$Box.Cox)
p5 <- ggplot(ing_tot) +
  geom_histogram(aes(x = Box.Cox, 
                     fill = "Box-Cox"), 
                 alpha = 0.5, fill = "purple", bins = 30) +
  geom_text(aes(x = Inf, y = Inf, hjust = 1, 
                vjust = 1, 
                label = paste("Skewness", round(sk_x5, 2)))) +
  theme_classic() +
  labs(x = "Transformacion Box-Cox de Ingresos totales", 
       y = "Cantidad")

ggarrange(p1, p2, p3, p4, p5, nrow = 3, ncol = 2)

##U otra opción es 

(p1 | p2 | p3) /
  (p4 | p5)


##En búsqueda de valores atípicos se realizan gráficos de cajas y bigotes y análisis de correlaciones

##Distribución horas trabajadas
d1 <- ggplot(geih_clean, aes(y = totalHoursWorked)) +
  geom_boxplot(fill = "darkslategray3", alpha = 0.5) +
  theme_classic() +
  labs(y = "Horas trabajadas") +
  scale_x_discrete( ) 

##Distribución ingresos
d2 <- ggplot(geih_clean, aes(y = ingtot)) +
  geom_boxplot(fill = "darkslategray3", alpha = 0.5) +
  theme_classic() +
  labs(y = "Ingresos totales") +
  scale_y_continuous(labels = scales::dollar) +
  scale_x_discrete( ) 
##Eliminamos una única observación que indica un ingreso total de $50 millones.
geih_clean <- geih_clean=geih[geih$ingtotes<40000000,]

##Distribución experiencia
d3 <- ggplot(geih_clean, aes(y = p6426)) +
  geom_boxplot(fill = "darkslategray3", alpha = 0.5) +
  theme_classic() +
  labs(y = "Años de Experiencia") +
  scale_y_continuous(labels = scales::dollar) +
  scale_x_discrete( ) 

##Distribución edad
d4 <- ggplot(geih_clean, aes(y = age)) +
  geom_boxplot(fill = "darkslategray3", alpha = 0.5) +
  theme_classic() +
  labs(y = "Edad") +
  scale_y_continuous(labels = scales::dollar) +
  scale_x_discrete( ) 

ggarrange(d1, d2, d3, d4, d5, nrow = 3, ncol = 2)
(d1 | d2) /
  (d3 | d4)

##Estadísticas descriptivas, tablas y figuras

#Variables de interés defindas: 

##Las variables que se considera pueden influir en el salario es si se encuentran en una zona urbana-rural,
##el sexo, el número de horas trabajadas, el nivel máximo de educación alcanzado, el oficio y la experiencia en el oficio actual
##(clase (urbano-rural), maxEducLevel (sim. P6210),  oficio, sex, totalHoursWorked, P6426 (experiencia en oficio actual))

#Distribución de categóricas
##Distribución de nivel educativo
g1 <- ggplot() +
  geom_bar(data = geih_clean, aes(x = p6210), 
           fill = "darkslategray3", alpha = 0.5) +
  labs(x = "Nivel educativo", y = "Frequencia") + 
  theme_classic()

g2 <- ggplot() +
  geom_bar(data = geih_clean, aes(x = maxEducLevel), 
           fill = "darkslategray3", alpha = 0.5) +
  labs(x = "Nivel Educativo Máximo", y = "Frequencia") + 
  theme_classic()

##Distribución por sexo
g3 <- ggplot() +
  geom_bar(data = geih_clean, aes(x = sex), 
           fill = "darkslategray3", alpha = 0.5) +
  labs(x = "Sexo", y = "Frequencia") + 
  theme_classic()
(g1 | g2) /
  (g3)
##Distribución por zona urbana-rural
ggplot() +
  geom_bar(data = geih_clean, aes(x = clase), 
           fill = "darkslategray3", alpha = 0.5) +
  labs(x = "Zona", y = "Frequencia") + 
  theme_classic()
##Todas las observaciones se encuentran en zona urbana, por lo que no se tendrá en cuenta

##Correlación ingresos y horas trabajadas
cor1 <- ggplot(geih_clean, aes(x = totalHoursWorked, y = ingtot)) +
  geom_point(color = "darkslategray3", alpha = 0.5) +
  theme_classic() +
  scale_y_continuous(labels = scales::dollar) +
  # scale_y_continuous(labels = scales::dollar, trans = 'log10') +
  labs(x = "Horas Trabajadas", y = "Ingresos totales (log)")
##Correlación ingresos y experiencia
cor2 <- ggplot(geih_clean, aes(x = p6426, y = ingtot)) +
  geom_point(color = "darkslategray3", alpha = 0.5) +
  theme_classic() +
  scale_y_continuous(labels = scales::dollar) +
  # scale_y_continuous(labels = scales::dollar, trans = 'log10') +
  labs(x = "Experiencia", y = "Ingresos totales (log)")

##Correlación ingresos y edad
cor3 <- ggplot(geih_clean, aes(x = age, y = ingtot)) +
  geom_point(color = "darkslategray3", alpha = 0.5) +
  theme_classic() +
  scale_y_continuous(labels = scales::dollar) +
  # scale_y_continuous(labels = scales::dollar, trans = 'log10') +
  labs(x = "Edad", y = "Ingresos totales (log)")

(cor1 | cor2) /
  (cor3)

#Tablas
##Se etiquetarán las variables para mayor claridad
geih_clean = apply_labels(geih_clean,
                      ingtot = "Ingreso Total",
                      totalHoursWorked = "Horas trabajadas",
                      age = "Edad",
                      p6426 = "Experiencia",
                      p6210 = "Nivel Educativo",
                      sex = "Sexo")
#Tablas descriptivas

vartable <- st(geih_clean, vars = c('ingtot','totalHoursWorked', 'p6426', 'age'), labels = c("Ingresos Totales", "Horas trabajadas", "Experiencia", "Edad"))

sumar1 <- describe(geih_clean[ , c('ingtot', 'age', 'totalHoursWorked', 'p6426')],fast=TRUE)

stargazer(geih_clean[c('ingtot', 'age', 'totalHoursWorked', 'p6426')], type = "text", 
                             digits=2, median = TRUE, iqr = TRUE,
                             title="Descriptive statistics",
                             covariate.labels=c("Ingreso total","Edad","Horas trabajadas","p6426"),out="table1.txt")

##Correlaciones
# Primero seleccionamos las columnas numéricas
#Se obtienen las correlaciones
correla <- geih_clean[c('ingtot', 'age', 'totalHoursWorked', 'p6426')]
mcor <- round(cor(correla[, unlist(lapply(correla, is.numeric))]),2)
#Se mantiene toda la tabla
upper<-mcor
upper[upper.tri(mcor)]<-""
upper<-as.data.frame(upper)

corrplot(cor(correla[, unlist(lapply(correla, is.numeric))]))


#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
#%    Age-earnings profile 		          %
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

##El ingreso total observado es  la suma de los ingresos percibidos en las siguientes fuentes: 
#ingreso monetario primera actividad (impa), ingreso segunda actividad (isa), ingreso en especie (ie), 
#ingreso monetario desocupados e inactivos (imdi) e ingresos provenientes de otras fuentes no laborales (iof) (intereses, pensiones, ayudas,
#cesantias, arriendos y otros) .

#El ingreso total imputado es la suma de cada una de las fuentes de ingresos 
#imputadas a los registros faltantes

#El ingreso total es la suma de cada una de las fuentes de ingresos
#tanto observadas como imputadas.

geih_clean <- geih_clean %>%
  mutate(geih_clean, age2=age^2)
modelOLS <- lm(ingtot ~ age+age2, data = geih_clean)
summary(modelOLS)
require("stargazer")
stargazer(modelOLS)

##R squared del 1.6%

length(predict(modelOLS)) == length(geih_clean$ingtot)
plot(predict(modelOLS), geih_clean$ingtot[1:length(predict(modelOLS))])
abline(a=0, b=1)

##Se limitan las observaciones para una mejor observación de la estimación predicha vs la observada
plot(x = predict(modelOLS), y = geih_clean$ingtot,
    xlab='Predicted Values',
    ylab='Actual Values',
    main='Predicted vs. Actual Values', 
    ylim = c(0, 5000000))
abline(a=0, b=1)

plot(y = predict(modelOLS), x = geih_clean$age,
     ylab='Predicted Values',
     xlab='Edad',
     main='Predicted Income by age')
abline(a=0, b=1)

# plot predicted values and actual values

##What is the “peak age” suggested by the above equation? Use bootstrap to calculate the standard errors and construct the confidence intervals.

require("tidyverse")
set.seed(1111)
R<-1000 # Number of Repetions
eta_mod1<-rep(0,R)

for (i in 1:R){
  geih_sample<- sample_frac(geih_clean,size=1,replace=TRUE)
  f <-lm(ingtot ~ age+age2, geih_sample)
  coefs<-f$coefficients
  eta_mod1[i]<-coefs[2]
}  
  plot(hist(eta_mod1), main='Histograma Bootstrap')
  mean(eta_mod1)
  sqrt(var(eta_mod1))
  quantile(eta_mod1,c(0.025,0.975))
  
##O con el comando boot 
  
  require("boot")
  Predictvalues <- function(geih_clean, index) {
    coef(lm(ingtot ~ age+age2, data = geih_clean, subset = index))
    
  } 
  
boot(geih_clean, Predictvalues, R = 1000)

beta <- coef(lm(ingtot ~ age+age2, data = geih_clean))
beta1 <- beta[1]
beta2 <- beta[2]
beta3 <- beta[3]

funcion <- expression(beta1+beta2*x+beta3*x^2)

derivada <- D(funcion,'x')
peak_age <- (beta2/(-beta3*2))
peak_age

#-----------Punto 4 GAP Earnings GAP-------------------
#-----------Punto 4-------------------
#Base a utilizar en este punto
geih_clean <- geih_sinna %>%
                select(ingtot, sex, age)

#Transformar el ingreso en logaritmo
geih_punto4 <- geih_punto4 %>%
                mutate(logIng = log(ingtot))

geih_punto4 <- geih_punto4 %>%
  filter(logIng > 0)

#Mirar las proporciones
table(geih_punto4$sex)


#Los 1 son hombres, hay que cambiar esto para hacer nuestro modelo
geih_punto4 <- geih_punto4 %>%
  mutate(female = ifelse(sex == "0", 1, 0))

geih_punto4 <- geih_punto4 %>%
  mutate(female = as.factor(female))

table(geih_punto4$female)

str(geih_punto4)
summary(geih_punto4)

#Correlación
M_p4 <- geih_punto4 %>%
          select(-sex) %>%
          cor()
M_p4 #No está tan clara la correlación

#Modelo lineal
#Estimar el modelo de gap
ModeloGap <- lm(logIng ~ female, data = geih_punto4)

geih_punto4 %>% ggplot(aes(x = logIng)) + geom_histogram()

summary(ModeloGap)

#El modelo muestra que la variable independiente es significativa y muestra que al ser mujer se reduce el ingreso en en un 19.6% Sin embargo,
#el R2 es solamente del 1.2%, lo que significa que la variable de género solo explica un 1.2% la varianza del modelo.
stargazer(ModeloGap, type = "text")


GapPredict <- predict(ModeloGap)

#Se incluyen los y estimados para comprarar el modelo
geih_punto4 <- geih_punto4 %>%
                mutate(fitValues = ModeloGap$fitted.values)

#Solo hay dos valores para los y estimados: uno  para mujer (13.97) y otro para hombre (14.07)
summary(geih_punto4)
exp(max(geih_punto4$fitValues))
exp(min(geih_punto4$fitValues))

#Analizamos algunas gráficas
ggplot(geih_punto4, aes(x = logIng, y = fitValues, colour = female)) + geom_point()          
ggplot(geih_punto4, aes(x = fitValues, y = logIng, colour = female)) + geom_point()
ggplot(geih_punto4, aes(x= logIng, fill = female)) + geom_histogram(position = "identity", alpha = 0.5)
ggplot(geih_punto4, aes(x= fitValues, fill = female)) + geom_histogram(position = "identity", alpha = 0.5)


#Modelo con edad y genero
geih_punto4 <- geih_punto4 %>%
                mutate(age2 = age^2)

ModeloGapAge <- lm(logIng ~ female + age + age2, data = geih_punto4)
summary(ModeloGapAge)
stargazer(ModeloGapAge, type = "text")

#Plot con los datos reales. Muestra que si tienen un mismo intercepto pero los ingresos divergen a lo largo del tiempo.
#Los hombres crecen y las mujeres decrecen
ggplot(geih_punto4, aes(x = age, y = logIng, colour = female)) + geom_point(alpha = 0.15) + geom_smooth(method = 'lm')

#Plot con los Y estimados. No hay intercepción, aunque muestran la misma pendiente a lo largo del gráfico. Llegan a un punto máximo
#Y empieza a decrecer
ggplot(geih_punto4, aes(x = age, y = ModeloGapAge$fitted.values, colour = female)) + geom_point(alpha = 0.15) + geom_smooth()

#Ahora se realiza la regresion con la interacción
geih_punto4$numfem <- as.numeric(geih_punto4$female)
geih_punto4$numfem <- ifelse(geih_punto4$numfem == 2, 1, 0)

geih_punto4$agefem <- geih_punto4$numfem*geih_punto4$age

#Los coeficientes cambian, hay un mayor impacto de género. EL R2 aumenta a 4,24%
ModeloGapAgeInt <- lm(logIng ~ female + age + age2 + agefem, data = geih_punto4)
coef(ModeloGapAgeInt)
summary(ModeloGapAgeInt)

#Plot con los Y estimados. No hay intercepción, aunque muestran la misma pendiente a lo largo del gráfico. Llegan a un punto máximo
#Y empieza a decrecer
ggplot(geih_punto4, aes(x = age, y = ModeloGapAgeInt$fitted.values, colour = female)) + geom_point(alpha = 0.15) + geom_smooth()


#Peak Age con Bootstrap



p_load(boot)

model_coef <- function(data, index){
  coef(lm(logIng ~ female + age + age2, data = data, subset = index)) #Crear la función para calcular los errores estándar
}
model_coef(geih_punto4, 1:16138) #Probar la función

ModeloBoot_AgeCAP2 <- boot(geih_punto4, model_coef, R=1000) #Correr el boot. Se puede ver que los errores estándar mejoraran con el boot

BootSE <- as.data.frame(ModeloBoot_AgeCAP2$t)

#Calculamos los intervalos de confianza para cada variable
hist(BootSE$V2)
IC_FMod1 <- quantile(BootSE$V2,c(0.025,0.975))

hist(BootSE$V3)
IC_AgeMod1 <- quantile(BootSE$V3,c(0.025,0.975))

hist(BootSE$V4)
IC_Age2Mod1 <- quantile(BootSE$V4,c(0.025,0.975))

#Ninguna de las variables tiene overlap en los intervalos

max(ModelosBoot_AgeCAP$finalModel$fitted.values)
max(ModeloGapAge$fitted.values)

geih_punto4$fitValuesGAPAGE <- ModelosBoot_AgeCAP$finalModel$fitted.values

#Peakage en la gráfica
        
#Para los dos géneros la edad de pico son los 44 años

peaked_age <- ModelosBoot_AgeCAP$finalModel$coefficients[3]/(-ModelosBoot_AgeCAP$finalModel$coefficients[4]*2)


#Bootstrap para interacción

model_coef2 <- function(data, index){
  coef(lm(logIng ~ female + age + age2 + agefem, data = data, subset = index)) #Crear la función para calcular los errores estándar
}
model_coef2(geih_punto4, 1:16138) #Probar la función

ModeloBoot_AgeFEM <- boot(geih_punto4, model_coef2, R=1000) #Correr el boot. Se puede ver que los errores estándar mejoraran con el boot
summary(ModeloBoot_AgeFEM)


#Calculamos los intervalos de confianza para cada variable
BootSEFAGE <- as.data.frame(ModeloBoot_AgeFEM$t)

#Calculamos los intervalos de confianza para cada variable
hist(BootSEFAGE$V2)
IC_FMod2 <- quantile(BootSEFAGE$V2,c(0.025,0.975))

hist(BootSEFAGE$V3)
IC_AgeMod2 <- quantile(BootSEFAGE$V3,c(0.025,0.975))

hist(BootSEFAGE$V4)
IC_Age2Mod2 <- quantile(BootSEFAGE$V4,c(0.025,0.975))

hist(BootSEFAGE$V5)
IC_AgeFemMod2 <- quantile(BootSEFAGE$V5,c(0.025,0.975))


#peak age
PeakAgeMujer <- (ModeloBoot_AgeFEM$t0[3] + ModeloBoot_AgeFEM$t0[5])/(-2*ModeloBoot_AgeFEM$t0[4])
#Las mujeres tienen su cúspide salarial a los 39.31 años

PeakAgeHombre <- ModeloBoot_AgeFEM$t0[3]/(-2*ModeloBoot_AgeFEM$t0[4])
#Los hombres, por su parte tienen su cúspide salarial a los 48.25 años




#--------------Punto 5----------------------------------------------

#a. Split the sample

#Incluir variables de los modelos anteriores
geih_clean$logIng <- log(geih_sinna$ingtot)
geih_clean$age2 <- (geih_sinna$age)^2
geih_clean$female <- ifelse(geih_sinna$sex == "0",1,0)
geih_clean$agefem <- geih_sinna$age*geih_sinna$female

#Separar la muestra:
set.seed(10102) #Sembrar la semilla para aleatorizar la muestra
#Dividir la muestra en 70% para entrenar y 30% para hacer el testeo
index <-  round(nrow(geih_clean)*0.3,digits=0)
#Muestra aleatorizada del df y mantener el número de observaciones del indice
test.indices <- sample(1:nrow(geih_clean), index)
# set de entrenamiento
geih.train<-geih_clean[-test.indices,] 
#30% set de testeo
geih.test<-geih_clean[test.indices,] 
#Seleccionar el set de entrenamiento en variables independientes y dependientes
YTrain <- geih.train$ingtot
XTrain <- geih.train %>% select(-ingtot)
#Seleccionar el set de testeo en variables independientes y dependientes
YTest <- geih.test$Diagnosis
XTest <- geih.test %>% select(-ingtot)

#Regresion de solo el intercepto
Modelobase <- lm(ingtot ~ 1, data = geih.train)
stargazer(Modelobase, type ="text")
mean(geih.train$ingtot)

#El intercepto es la media de los ingresos totales de las personas
#Ahora estimamos nuestros modelos anteriores
#Modelo Edad
ModeloEdad <- lm(ingtot ~ age + age2, data = geih.train)

#Modelo GAP y edad
#Filtramos los logaritmos mayores a 0
geih.train2 <- geih.train %>%
                filter(logIng > 0)

ModeloFemale <- lm(logIng ~ female, data = geih.train2)
ModeloAgeGAP <- lm(logIng ~ female + age + age2 + agefem, data = geih.train2)

stargazer(Modelobase, ModeloEdad, ModeloFemale, ModeloAgeGAP, type = "text")

